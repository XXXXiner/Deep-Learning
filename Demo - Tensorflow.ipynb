{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XXXXiner/Deep-Learning/blob/main/tensorflow_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nYov1nvy2ek"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TensorFlow version:\", tf.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbIK6vAM7lSs",
        "outputId": "2540b7c7-424b-444a-86d8-22d0ad676aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of standard Python data structures, Tensorflow uses “tensors.”\n",
        "\n",
        "To convert a Python objects of type list, int, str, etc; use tf.constant\n"
      ],
      "metadata": {
        "id": "yR8vbMtZzHc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.constant([1.0, 2.0, 3.0])"
      ],
      "metadata": {
        "id": "SARA32xkzQWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.constant('hello world')"
      ],
      "metadata": {
        "id": "OCNCyG7RzdcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use .numpy() to get a numpy value from a tensor"
      ],
      "metadata": {
        "id": "nckCHtxxzlo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.constant('hello world').numpy() #.decode() #to get the string"
      ],
      "metadata": {
        "id": "szlKrmi-zrrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random is useful for generating weights and bias matrices"
      ],
      "metadata": {
        "id": "4vGugB850k21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.random.normal([2, 2])\n",
        "W = tf.random.uniform([2, 2])\n",
        "print(x)\n",
        "print(W)"
      ],
      "metadata": {
        "id": "DqYmxZes04YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Addition and multiplication are overloaded (a feature that allows a class to have more than one method with the same name) in Tensorflow"
      ],
      "metadata": {
        "id": "YK9CWYBa1Fob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.add(x, W))\n",
        "print(x + W)"
      ],
      "metadata": {
        "id": "TS3xqBet0_yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.multiply(x, W))\n",
        "print(x * W)"
      ],
      "metadata": {
        "id": "PXa_yNa-1W82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remember:** tf.multiply is **not** matrix multiplication, but elementwise multiplication. For dense layers, use tf.matmul:"
      ],
      "metadata": {
        "id": "6_uGYWrJ1jzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.matmul(x, W)"
      ],
      "metadata": {
        "id": "FArTm-7O1dQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Useful operations in TF**"
      ],
      "metadata": {
        "id": "WCTUP4jA4vtF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** that many tensorflow operations are the same as numpy operations.\n",
        "\n",
        "This does not mean that you should use numpy operations on tensors anywhere in your model: this will break autodiff. Only use numpy operations in data preprocessing or in evaluation.\n"
      ],
      "metadata": {
        "id": "eqHITWZZ10i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Argmax** is useful for finding the maximum value logit when making predictions"
      ],
      "metadata": {
        "id": "mkjIszRh2ESZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)\n",
        "print(tf.argmax(x))\n",
        "print(np.argmax(x))"
      ],
      "metadata": {
        "id": "s2C1P8-Y19IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transpose** will be used in later assigments. The transpose of a matrix is that same matrix with the rows and columns switched (https://en.wikipedia.org/wiki/Transpose)"
      ],
      "metadata": {
        "id": "h-QKmCYG3Kji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.transpose(x))\n",
        "print(np.transpose(x))"
      ],
      "metadata": {
        "id": "9lrAppdy3Jac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shape** is useful for getting the dimensions of your tensor. This often comes in handy when debugging"
      ],
      "metadata": {
        "id": "yOiDIX2t0h3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.shape(x))\n",
        "print(np.shape(x))"
      ],
      "metadata": {
        "id": "Rd1rHyli4jmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reduce_mean** will take the mean along the axis specified, or along all values if no axis is specified. This is useful in loss calculations."
      ],
      "metadata": {
        "id": "V60JzNN84s2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.reduce_mean(x)) #similar idea for sum (reduce_sum)\n",
        "print(np.mean(x))"
      ],
      "metadata": {
        "id": "JXr5r3NV4sYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reshape** is useful for changing the dimension of your data."
      ],
      "metadata": {
        "id": "QHkoBuRQ5l2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.reshape(x, [1,4])) #a shape of [-1] flattens into 1-D\n",
        "print(np.reshape(x, [1,4]))"
      ],
      "metadata": {
        "id": "I3lAVvCW4oop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variables are Tensors that can change (or be updated). This makes them useful for our weight and bias matrices (or parameters) for the model."
      ],
      "metadata": {
        "id": "oN_2LNqJ6kRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable([2.1, 3.5, 9.1])\n",
        "b = tf.Variable([1.0, 0.2, 0.3])\n",
        "\n",
        "tf.add(x,b)"
      ],
      "metadata": {
        "id": "qjy96O__6IoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building a neural network**"
      ],
      "metadata": {
        "id": "kDWrtxsO7GRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've got the basics out of the way, let's see how to define a one-layer neural network model for classifying MNIST digits:"
      ],
      "metadata": {
        "id": "AZ2D4bnm7KAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load MNIST data as training and test sets\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-6wJ5KU7JGI",
        "outputId": "dc42c5a7-c5c0-4b22-ccee-7e35dec28896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000,)\n",
            "(10000, 28, 28) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape data to make it one-dimensional and normalize it to scale between 0 and 1\n",
        "x_train = tf.reshape(x_train / 255.0, [-1,784])\n",
        "x_test = tf.reshape(x_test / 255.0, [-1,784])\n",
        "\n",
        "#set batch size\n",
        "batch_size=500"
      ],
      "metadata": {
        "id": "JUmy_IlH780E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model():\n",
        "  def __init__(self):\n",
        "    # declare weights and bias matrices\n",
        "    self.W = tf.Variable(tf.random.uniform([784,10], -1, 1, dtype=tf.float64))\n",
        "    self.b = tf.Variable(tf.random.uniform([10], -1, 1, dtype=tf.float64))\n",
        "\n",
        "  def get_logits(self, x):\n",
        "    return tf.matmul(x,self.W) + self.b\n",
        "\n",
        "  def get_loss(self, data, label):\n",
        "    # getting mean loss across examples\n",
        "    return tf.reduce_mean(\n",
        "      # crossentropy where the label = index of correct answer in logits\n",
        "      # from_logits = apply softmax\n",
        "      tf.losses.SparseCategoricalCrossentropy(from_logits=True)(\n",
        "          label, self.get_logits(data)\n",
        "      )\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "HedQgTr98T1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that we use random numbers to initialize our weights and biases. Why do we do that? Why not initialize them to zero, like we did with the perceptron algorithm? Think about how gradient descent works: if all of the weights start out with the same value (i.e. zero), then the derivative of the loss with respect to each weight will be the same. The consequence of this: the gradient updates for all the weights will be the same, so all the weights will end up with the exact same value! This is clearly not good behavior--it reduces the power of our neural network.\n",
        "\n",
        "To keep this from happening, we initialize the weights with small random values, so that their derivatives are all different, and gradient descent can then push them in different directions.\n",
        "\n",
        "In the code above, we also randomly initialize the biases. This is not strictly necessary (randomly initializing the weights alone is enough to get around the problem described earlier), but it's common practice.\n"
      ],
      "metadata": {
        "id": "OqvEeW3e9HS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how we train the model. Note the use of tf.GradientTape(); this is a data structure that records the computation graph of code we execute, so that we can walk that graph backward and compute gradients."
      ],
      "metadata": {
        "id": "CL6L27Sz9Tvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# declare model to setup variables\n",
        "model = Model()\n",
        "\n",
        "# use stochastic gradient descent\n",
        "optimizer = tf.optimizers.SGD()\n",
        "\n",
        "def train(inputs, outputs):\n",
        "    # use gradient tape to record the loss calculation\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss = model.get_loss(inputs, outputs)\n",
        "\n",
        "    # use tape.gradient to retrieve δ(loss) / δ(weight_i), etc\n",
        "    grads = tape.gradient(loss, [model.W, model.b])\n",
        "    # apply the gradients to our weights and bias\n",
        "    optimizer.apply_gradients(zip(grads, [model.W, model.b]))\n",
        "\n",
        "# step through data one batch at a time, and apply training step\n",
        "\n",
        "for x in range(0, x_train.shape[0], batch_size):\n",
        "  print(f'processed {x}/{x_train.shape[0]}')\n",
        "  train(x_train[x: x + batch_size, :], y_train[x: x + batch_size])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ukpJTB5f9F64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, you can use 'optimizer.minimize,' which is syntax sugar that encapsulates all the gradient tape business above:"
      ],
      "metadata": {
        "id": "m6bTxU9P-ND5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()\n",
        "\n",
        "optimizer = tf.optimizers.SGD()\n",
        "\n",
        "def train(inputs, outputs):\n",
        "    # .minimize() applies all the steps in our train function\n",
        "    optimizer.minimize(\n",
        "      lambda: model.get_loss(inputs, outputs),\n",
        "      [model.W, model.b]\n",
        "    )\n",
        "\n",
        "\n",
        "for x in range(0, x_train.shape[0], batch_size):\n",
        "  print(f'processed {x}/{x_train.shape[0]}')\n",
        "  train(x_train[x: x + batch_size, :], y_train[x: x + batch_size])\n"
      ],
      "metadata": {
        "id": "byR7WbKF8_Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can evaluate the loss on our test set, as well as the prediction accuracy on our test set:"
      ],
      "metadata": {
        "id": "BWybRHfd-WtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = model.get_loss(x_test, y_test)\n",
        "print(loss)\n",
        "\n",
        "pred = np.argmax(model.get_logits(x_test), axis=1)\n",
        "acc = np.mean(pred == y_test)\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "U6qzuYs6-ZFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acknowledgements: This demo is a modified version of the one originally created by Daniel Ritchie"
      ],
      "metadata": {
        "id": "Jb8zrhT9wID5"
      }
    }
  ]
}